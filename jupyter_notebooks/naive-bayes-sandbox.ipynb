{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Implementations\n",
    "Although I plan to use real-world data to test the naive bayes classifier algorithm I'm building, \n",
    "it will likely be far quicker to use toy datasets to experiment with the Multinomial and Gaussian flavors of \n",
    "naive bayes, which is the purpose of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes\n",
    "For the multinomial version of naive bayes, I need a dataset that contains discrete features, such as counts. I'll be using a dataset of IMDB reviews labelled either positive or negative. The labelled data is a text file with a 1 or 0 at the end of the line denoting a positive or negative review, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While you don't yet hear Mickey speak, there are tons of sound effects and music throughout the film--something we take for granted now but which was a huge crowd pleaser in 1928.  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a very very very slow moving aimless movie abo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not sure who was more lost the flat characters...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attempting artiness with black white and cleve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>very little music or anything to speak of</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the best scene in the movie was when gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  a very very very slow moving aimless movie abo...          0\n",
       "1  not sure who was more lost the flat characters...          0\n",
       "2  attempting artiness with black white and cleve...          0\n",
       "3          very little music or anything to speak of          0\n",
       "4  the best scene in the movie was when gerardo i...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I have removed punctuation and excess whitespace to\n",
    "# prevent certain words from being differentiated, such\n",
    "# as \"very,\" and \"very\"\n",
    "\n",
    "imdb_cols = [\"review\", \"sentiment\"]\n",
    "imdb = pd.read_csv(\"imdb_labelled.txt\", sep=\"\\t\", names=imdb_cols)\n",
    "print(imdb[\"review\"][344])\n",
    "\n",
    "imdb[\"review\"] = imdb[\"review\"].str.strip()\n",
    "imdb[\"review\"] = imdb[\"review\"].str.replace(r\"[^\\w\\s-]\", \"\")\n",
    "imdb[\"review\"] = imdb[\"review\"].str.replace(r\"\\-\", \" \")\n",
    "imdb[\"review\"] = imdb[\"review\"].str.replace(r\"\\s{2,}\", \" \")\n",
    "imdb[\"review\"] = imdb[\"review\"].str.lower()\n",
    "\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "while you dont yet hear mickey speak there are tons of sound effects and music throughout the film something we take for granted now but which was a huge crowd pleaser in 1928\n"
     ]
    }
   ],
   "source": [
    "print(imdb[\"review\"][344])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(598, 3114)\n",
      "(150, 3114)\n",
      "(598,)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "# use sklearn's count vectorizer to create vectors for each review\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(imdb[\"review\"])\n",
    "y = imdb[\"sentiment\"]\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.20, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of being a good review is $\\frac{312}{598}$, while the probability of being a bad review is $\\frac{286}{598}$\n",
    "\n",
    "In other words:\n",
    "\n",
    "$P(Good) \\approx 0.522$\n",
    "\n",
    "$P(Bad) \\approx 0.478$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    312\n",
      "0    286\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.521739\n",
       "0    0.478261\n",
       "Name: sentiment, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determining the prior probabilities for good and bad reviews\n",
    "print(y_train.value_counts())\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to examine the probability of a particular word being in a bad review. In this case, I'll be looking at the word \"bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>010</th>\n",
       "      <th>10</th>\n",
       "      <th>1010</th>\n",
       "      <th>110</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>17</th>\n",
       "      <th>18th</th>\n",
       "      <th>1928</th>\n",
       "      <th>...</th>\n",
       "      <th>your</th>\n",
       "      <th>youre</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youthful</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youve</th>\n",
       "      <th>yun</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombiez</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   010  10  1010  110  12  13  15  17  18th  1928  ...  your  youre  yourself  \\\n",
       "0    0   0     0    0   0   0   0   0     0     0  ...     0      0         0   \n",
       "1    0   0     0    0   0   0   0   0     0     0  ...     0      0         0   \n",
       "2    0   0     0    0   0   0   0   0     0     0  ...     0      0         0   \n",
       "3    0   0     0    0   0   0   0   0     0     0  ...     0      0         0   \n",
       "4    0   0     0    0   0   0   0   0     0     0  ...     0      0         0   \n",
       "\n",
       "   youthful  youtube  youve  yun  zillion  zombie  zombiez  \n",
       "0         0        0      0    0        0       0        0  \n",
       "1         0        0      0    0        0       0        0  \n",
       "2         0        0      0    0        0       0        0  \n",
       "3         0        0      0    0        0       0        0  \n",
       "4         0        0      0    0        0       0        0  \n",
       "\n",
       "[5 rows x 3114 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a matrix with \"feature names\"\n",
    "words = vectorizer.get_feature_names()\n",
    "term_matrix = pd.DataFrame(X_train.toarray(), columns=words)\n",
    "term_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>010</th>\n",
       "      <th>10</th>\n",
       "      <th>1010</th>\n",
       "      <th>110</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>17</th>\n",
       "      <th>18th</th>\n",
       "      <th>1928</th>\n",
       "      <th>...</th>\n",
       "      <th>youre</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youthful</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youve</th>\n",
       "      <th>yun</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombiez</th>\n",
       "      <th>review_of_movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   010  10  1010  110  12  13  15  17  18th  1928  ...  youre  yourself  \\\n",
       "0    0   0     0    0   0   0   0   0     0     0  ...      0         0   \n",
       "1    0   0     0    0   0   0   0   0     0     0  ...      0         0   \n",
       "2    0   0     0    0   0   0   0   0     0     0  ...      0         0   \n",
       "3    0   0     0    0   0   0   0   0     0     0  ...      0         0   \n",
       "4    0   0     0    0   0   0   0   0     0     0  ...      0         0   \n",
       "\n",
       "   youthful  youtube  youve  yun  zillion  zombie  zombiez  review_of_movie  \n",
       "0         0        0      0    0        0       0        0                1  \n",
       "1         0        0      0    0        0       0        0                0  \n",
       "2         0        0      0    0        0       0        0                0  \n",
       "3         0        0      0    0        0       0        0                0  \n",
       "4         0        0      0    0        0       0        0                0  \n",
       "\n",
       "[5 rows x 3115 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the movie review label for reference\n",
    "term_matrix[\"review_of_movie\"] = y_train.values\n",
    "term_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in bad: 6003\n",
      "Total words in good: 5954\n"
     ]
    }
   ],
   "source": [
    "# get all bad and good reviews\n",
    "bad = term_matrix[term_matrix[\"review_of_movie\"] == 0]\n",
    "good = term_matrix[term_matrix[\"review_of_movie\"] == 1]\n",
    "\n",
    "# total number of words in each\n",
    "print(f\"Total words in bad: {bad.sum().sum()}\")\n",
    "print(f\"Total words in good: {good.sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times does the word bad occur in good and bad reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total times 'bad' appears in bad reviews: 50\n",
      "Total times 'bad' appears in good reviews: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total times 'bad' appears in bad reviews: {bad.bad.sum()}\")\n",
    "print(f\"Total times 'bad' appears in good reviews: {good.bad.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability that we will observe the word \"bad\" given that it was seen in a bad review is $\\frac{50}{6003}$, while the probability that you might observe it in a good review is $\\frac{7}{5954}$. That is:\n",
    "\n",
    "$P(bad|Bad) \\approx 0.008$\n",
    "\n",
    "$P(bad|Good) \\approx 0.001$\n",
    "\n",
    "These numbers will need to be acquired for each word and label using the numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11645"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[(y_train.values == 1)].toarray().sum() + X_train[(y_train.values == 0)].toarray().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11645"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix.sum().sum() - 312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing numpy's boolean indexing to see if\n",
    "# it works the way I think it does\n",
    "test_x = np.array([[0, 1, 1],\n",
    "                   [1, 2, 1],\n",
    "                   [6, 3, 1],\n",
    "                   [1, 4, 1]])\n",
    "test_y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# testing numpy's sum functions\n",
    "test_x[(test_y == 1)].sum(axis=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to know how calculating probabilities is going to work without using pandas explicitly. _Note_: The `y_train` values are a pandas series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(bad|Bad): 0.008\n",
      "P(good|Bad): 0.001\n"
     ]
    }
   ],
   "source": [
    "# number of times \"bad\" appears in bad reviews\n",
    "bad_in_bad = X_train[y_train.values == 0].toarray().sum(axis=0)[238]\n",
    "\n",
    "# number of times \"bad\" appears in good reviews\n",
    "bad_in_good = X_train[y_train.values == 1].toarray().sum(axis=0)[238]\n",
    "\n",
    "\n",
    "total_words_in_good = X_train[y_train.values == 1].sum()\n",
    "total_words_in_bad = X_train[y_train.values == 0].sum()\n",
    "\n",
    "print(\"P(bad|Bad): %.3f\" % (bad_in_bad / total_words_in_bad))\n",
    "print(\"P(good|Bad): %.3f\" % (bad_in_good / total_words_in_good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation above is the same as what I arrived at previously, using pandas. Now I need to figure out how to get and store these probabilities for each word, for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first I need to be able to count the classes in the dependent variable\n",
    "classes, counts = np.unique(y_train.values, return_counts=True)\n",
    "dict(zip(classes, counts))\n",
    "\n",
    "# I need to be able to store the individual probabilites for \n",
    "# each word, given a class\n",
    "class_probabilities = {c:{} for c in classes}\n",
    "class_probabilities\n",
    "\n",
    "for c in class_probabilities:\n",
    "    total_words = X_train[y_train.values == c].sum()\n",
    "    for w in range(X_train.shape[1]):\n",
    "        word_occurrences = X_train[y_train.values == c].toarray().sum(axis=0)[w]\n",
    "    \n",
    "        class_probabilities[c][w] = (word_occurrences / total_words)\n",
    "    \n",
    "class_probabilities        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word \"bad\" was the 238th word of the transposed term matrix. This should align with the new `class_probabilites` dictionary, which contains the probabilites for each word, given each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(bad|Bad): 0.008\n",
      "P(bad|Bad): 0.001\n"
     ]
    }
   ],
   "source": [
    "print(\"P(bad|Bad): %.3f\" % class_probabilities[0][238])\n",
    "print(\"P(bad|Bad): %.3f\" % class_probabilities[1][238])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the probabilities are reliably stored and indexed we should be able to use use Bayes' Theorem with naive assumptions (that is, assuming that each word is independent of all others. We're assuming no word affects the amount or appearance of any other word, so they affect probabilities independently) to classify a fake review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_review = [\"dont regret seeing this movie it was actually pretty good\"]\n",
    "fake_review_transformed = vectorizer.transform(fake_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "[  71  788 1196 1467 1791 2082 2189 2351 2713 2976]\n"
     ]
    }
   ],
   "source": [
    "for i in fake_review_transformed:\n",
    "    print(i.data)\n",
    "    print(i.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to classify the fake review, we need P(Bad) and P(Good)\n",
    "p_bad = counts[0] / (counts[0] + counts[1])\n",
    "p_good = counts[1] / (counts[0] + counts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, because there are zeros in the fake review, as well as in the document term matrix, the probability will likely come out to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "p = p_bad\n",
    "for i in fake_review_transformed.toarray()[0]:\n",
    "    p *= class_probabilities[0][i]\n",
    "    \n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in mind, smoothing will need to be incorporated by default. This will be accomplished by adding 1 to every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the probabilities need to be recalculated after adding 1 to every word\n",
    "# and increasing the total words by the number of total additions\n",
    "class_probabilities_smooth = {c:{} for c in classes}\n",
    "class_probabilities_smooth\n",
    "\n",
    "for c in class_probabilities_smooth:\n",
    "    total_words = X_train[y_train.values == c].sum() + X_train.shape[1]\n",
    "    for w in range(X_train.shape[1]):\n",
    "        word_occurrences = X_train[y_train.values == c].toarray().sum(axis=0)[w] + 1\n",
    "    \n",
    "        class_probabilities_smooth[c][w] = (word_occurrences / total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p_bad\n",
    "fake_array = zip(fake_review_transformed.data, fake_review_transformed.indices)\n",
    "for i, w in fake_array:\n",
    "    if i > 1:\n",
    "        p *= (class_probabilities_smooth[0][w]**i)\n",
    "        print(i)\n",
    "    else:\n",
    "        p *= class_probabilities_smooth[0][w]\n",
    "prob_bad = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p_good\n",
    "fake_array = zip(fake_review_transformed.data, fake_review_transformed.indices)\n",
    "for i, w in fake_array:\n",
    "    if i > 1:\n",
    "        p *= (class_probabilities_smooth[1][w]**i)\n",
    "        print(i)\n",
    "    else:\n",
    "        p *= class_probabilities_smooth[1][w]\n",
    "prob_good = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Review\n"
     ]
    }
   ],
   "source": [
    "if prob_good > prob_bad:\n",
    "    print(\"Good Review\")\n",
    "else:\n",
    "    print(\"Bad Review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all intents and purposes, the algorithm works well enough, but the \"fitting\" of the data takes entirely too long. How to solve this issue? Taking a peek at scikit-learn's [implementation of naive](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/naive_bayes.py) bayes exposes a clever mathematical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes in the example: [0 1]\n",
      "Binarized Classes: \n",
      "[[0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# First, the labels are binarized to provide\n",
    "# a \"one-vs-all\" method. This will help to \n",
    "# gather conditional probabilities in one location\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "Y = lb.fit_transform(y_train)\n",
    "Y = np.concatenate((1-Y, Y), axis=1)\n",
    "print(f\"Classes in the example: {lb.classes_}\")\n",
    "print(f\"Binarized Classes: \\n{Y[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was sklearn's version, the following is my implementation, which is probably simpler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: [1 2 3 4 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labs = np.array([1, 2, 4, 3, 5, 5, 4, 3, 2, 1 ,3, 4, 1])\n",
    "cls_ = np.unique(labs)\n",
    "lbins = np.zeros((labs.shape[0], np.unique(labs).shape[0]))\n",
    "for i in range(len(labs)):\n",
    "    x, = np.where(cls_ == labs[i])\n",
    "    lbins[i][x] = 1\n",
    "    \n",
    "print(f\"Unique classes: {cls_}\")\n",
    "lbins[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previously binarized `Y` for convenience, the features and classes can be counted by getting the dot product of the transposed `Y` and the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_count = np.dot(Y.T, X_train.toarray())\n",
    "class_count = Y.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number at index 238 for 'bad' (Bad Reviews): 50\n",
      "Number at index 238 for 'bad' (Good Reviews): 7\n"
     ]
    }
   ],
   "source": [
    "# is index 238 of the \"bad\" reviews (index 0 of Y)\n",
    "# equal to 50 occurences of the word bad? What\n",
    "# about the \"good\" reviews? Should be 7 occurences\n",
    "\n",
    "print(f\"Number at index 238 for 'bad' (Bad Reviews): {feature_count[0][238]}\")\n",
    "print(f\"Number at index 238 for 'bad' (Good Reviews): {feature_count[1][238]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the system seems trustworthy and getting the data into the respective data structures happens far quicker this way than the $O(n^2)$ solution\n",
    "from before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the conditional probabilities need to be extracted from the new arrays. This necessitates smoothing (adding 1 or a smaller value to all the feature counts) to avoid getting probabilities of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_feature_count = feature_count + 1\n",
    "\n",
    "# the smoothed class count sums word occurences\n",
    "# in each class array of Y and the total vocabulary\n",
    "# or occurrences, in this case 3114\n",
    "smoothed_instance_count = smoothed_feature_count.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since underflow is a real possibility when multiplying these probabilities, I'll once again take a page out of the sklearn book and perform arithmetic with the log function. This means instead of dividing the word counts by the total words in the class, I'll need to subtract the log class count from the smoothed feature count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.186070448860576\n",
      "-5.186070448860577\n"
     ]
    }
   ],
   "source": [
    "log_probs = (np.log(smoothed_feature_count) - \n",
    "             np.log(smoothed_instance_count.reshape(-1,1)))\n",
    "\n",
    "print(log_probs[0][238])  \n",
    "print(np.log(class_probabilities_smooth[0][238]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.73759894 -0.65058757]]\n",
      "[[-0.73759894 -0.65058757]]\n"
     ]
    }
   ],
   "source": [
    "# setting the log priors involves a similar process\n",
    "log_priors = np.log(class_count) - np.log(class_count.sum(axis=0).reshape(-1,1))\n",
    "print(np.log(class_count / class_count.sum(axis=0).reshape(-1,1)))\n",
    "print(log_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argmax function maximizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.dot(fake_review_transformed.toarray(), log_probs.T) + log_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if there are multiple reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-52.57164459 -51.68864253]\n",
      " [-66.78447684 -71.93680231]\n",
      " [-41.92879856 -41.64429618]]\n"
     ]
    }
   ],
   "source": [
    "multiple_reviews = [\"the best movie i have seen this year loved it\",\n",
    "                    \"awful just so slow and painful to watch, dont see it\",\n",
    "                    \"great movie enjoyed every minute almost cried\"]\n",
    "\n",
    "mr_transformed = vectorizer.transform(multiple_reviews)\n",
    "\n",
    "# \"add\" the log probabilities for every word in the review\n",
    "# by using the dot product approach, and then add the\n",
    "# log priors for each column respectively\n",
    "scores = np.dot(mr_transformed.toarray(), log_probs.T) + log_priors\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an array of predictions based on largest\n",
    "# score for each vector\n",
    "np.argmax(scores, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process results in practically the same outcome, but with a quicker \"fitting\" thanks to numpy math operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "For the Gaussian Naive Bayes, i'll be using the famed Iris dataset to provide continuous data for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Unit4-Sprint1-NLP (python3)",
   "language": "python",
   "name": "u4s1nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
