{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Implementations\n",
    "Although I plan to use real-world data to test the naive bayes classifier algorithm I'm building, \n",
    "it will likely be far quicker to use toy datasets to experiment with the Multinomial and Gaussian flavors of \n",
    "naive bayes, which is the purpose of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes\n",
    "For the multinomial version of naive bayes, I need a dataset that contains discrete features, such as counts. I'll be using a dataset of IMDB reviews labelled either positive or negative. The labelled data is a text file with a 1 or 0 at the end of the line denoting a positive or negative review, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While you don't yet hear Mickey speak, there are tons of sound effects and music throughout the film--something we take for granted now but which was a huge crowd pleaser in 1928.  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a very very very slow moving aimless movie abo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not sure who was more lost the flat characters...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attempting artiness with black white and cleve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>very little music or anything to speak of</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the best scene in the movie was when gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  a very very very slow moving aimless movie abo...          0\n",
       "1  not sure who was more lost the flat characters...          0\n",
       "2  attempting artiness with black white and cleve...          0\n",
       "3          very little music or anything to speak of          0\n",
       "4  the best scene in the movie was when gerardo i...          1"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I have removed punctuation and excess whitespace to\n",
    "# prevent certain words from being differentiated, such\n",
    "# as \"very,\" and \"very\"\n",
    "\n",
    "imdb_cols = [\"review\", \"sentiment\"]\n",
    "imdb = pd.read_csv(\"imdb_labelled.txt\", sep=\"\\t\", names=imdb_cols)\n",
    "print(imdb[\"review\"][344])\n",
    "\n",
    "imdb[\"review\"] = imdb[\"review\"].str.strip()\n",
    "imdb[\"review\"] = imdb[\"review\"].str.replace(r\"[^\\w\\s-]\", \"\")\n",
    "imdb[\"review\"] = imdb[\"review\"].str.replace(r\"\\-\", \" \")\n",
    "imdb[\"review\"] = imdb[\"review\"].str.replace(r\"\\s{2,}\", \" \")\n",
    "imdb[\"review\"] = imdb[\"review\"].str.lower()\n",
    "\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imdb[\"review\"][344])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sklearn's count vectorizer to create vectors for each review\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(imdb[\"review\"])\n",
    "y = imdb[\"sentiment\"]\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.20, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of being a good review is $\\frac{312}{598}$, while the probability of being a bad review is $\\frac{286}{598}$\n",
    "\n",
    "In other words:\n",
    "\n",
    "$P(Good) \\approx 0.522$\n",
    "\n",
    "$P(Bad) \\approx 0.478$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining the prior probabilities for good and bad reviews\n",
    "print(y_train.value_counts())\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to examine the probability of a particular word being in a bad review. In this case, I'll be looking at the word \"bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix with \"feature names\"\n",
    "words = vectorizer.get_feature_names()\n",
    "term_matrix = pd.DataFrame(X_train.toarray(), columns=words)\n",
    "term_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the movie review label for reference\n",
    "term_matrix[\"review_of_movie\"] = y_train.values\n",
    "term_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all bad and good reviews\n",
    "bad = term_matrix[term_matrix[\"review_of_movie\"] == 0]\n",
    "good = term_matrix[term_matrix[\"review_of_movie\"] == 1]\n",
    "\n",
    "# total number of words in each\n",
    "print(f\"Total words in bad: {bad.sum().sum()}\")\n",
    "print(f\"Total words in good: {good.sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times does the word bad occur in good and bad reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total times 'bad' appears in bad reviews: {bad.bad.sum()}\")\n",
    "print(f\"Total times 'bad' appears in good reviews: {good.bad.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability that we will observe the word \"bad\" given that it was seen in a bad review is $\\frac{50}{6003}$, while the probability that you might observe it in a good review is $\\frac{7}{5954}$. That is:\n",
    "\n",
    "$P(bad|Bad) \\approx 0.008$\n",
    "\n",
    "$P(bad|Good) \\approx 0.001$\n",
    "\n",
    "These numbers will need to be acquired for each word and label using the numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[(y_train.values == 1)].toarray().sum() + X_train[(y_train.values == 0)].toarray().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_matrix.sum().sum() - 312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing numpy's boolean indexing to see if\n",
    "# it works the way I think it does\n",
    "test_x = np.array([[0, 1, 1],\n",
    "                   [1, 2, 1],\n",
    "                   [6, 3, 1],\n",
    "                   [1, 4, 1]])\n",
    "test_y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# testing numpy's sum functions\n",
    "test_x[(test_y == 1)].sum(axis=0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to know how calculating probabilities is going to work without using pandas explicitly. _Note_: The `y_train` values are a pandas series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times \"bad\" appears in bad reviews\n",
    "bad_in_bad = X_train[y_train.values == 0].toarray().sum(axis=0)[238]\n",
    "\n",
    "# number of times \"bad\" appears in good reviews\n",
    "bad_in_good = X_train[y_train.values == 1].toarray().sum(axis=0)[238]\n",
    "\n",
    "\n",
    "total_words_in_good = X_train[y_train.values == 1].sum()\n",
    "total_words_in_bad = X_train[y_train.values == 0].sum()\n",
    "\n",
    "print(\"P(bad|Bad): %.3f\" % (bad_in_bad / total_words_in_bad))\n",
    "print(\"P(good|Bad): %.3f\" % (bad_in_good / total_words_in_good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation above is the same as what I arrived at previously, using pandas. Now I need to figure out how to get and store these probabilities for each word, for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first I need to be able to count the classes in the dependent variable\n",
    "classes, counts = np.unique(y_train.values, return_counts=True)\n",
    "dict(zip(classes, counts))\n",
    "\n",
    "# I need to be able to store the individual probabilites for \n",
    "# each word, given a class\n",
    "class_probabilities = {c:{} for c in classes}\n",
    "class_probabilities\n",
    "\n",
    "for c in class_probabilities:\n",
    "    total_words = X_train[y_train.values == c].sum()\n",
    "    for w in range(X_train.shape[1]):\n",
    "        word_occurrences = X_train[y_train.values == c].toarray().sum(axis=0)[w]\n",
    "    \n",
    "        class_probabilities[c][w] = (word_occurrences / total_words)\n",
    "    \n",
    "class_probabilities        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word \"bad\" was the 238th word of the transposed term matrix. This should align with the new `class_probabilites` dictionary, which contains the probabilites for each word, given each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"P(bad|Bad): %.3f\" % class_probabilities[0][238])\n",
    "print(\"P(bad|Bad): %.3f\" % class_probabilities[1][238])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the probabilities are reliably stored and indexed we should be able to use use Bayes' Theorem with naive assumptions (that is, assuming that each word is independent of all others. We're assuming no word affects the amount or appearance of any other word, so they affect probabilities independently) to classify a fake review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_review = [\"dont regret seeing this movie it was actually pretty good\"]\n",
    "fake_review_transformed = vectorizer.transform(fake_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 1 1 1]\n",
      "[ 226  293  493 1015 1791 1839]\n"
     ]
    }
   ],
   "source": [
    "for i in fake_review_transformed:\n",
    "    print(i.data)\n",
    "    print(i.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to classify the fake review, we need P(Bad) and P(Good)\n",
    "p_bad = counts[0] / (counts[0] + counts[1])\n",
    "p_good = counts[1] / (counts[0] + counts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, because there are zeros in the fake review, as well as in the document term matrix, the probability will likely come out to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p_bad\n",
    "for i in fake_review_transformed.toarray()[0]:\n",
    "    p *= class_probabilities[0][i]\n",
    "    \n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in mind, smoothing will need to be incorporated by default. This will be accomplished by adding 1 to every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the probabilities need to be recalculated after adding 1 to every word\n",
    "# and increasing the total words by the number of total additions\n",
    "class_probabilities_smooth = {c:{} for c in classes}\n",
    "class_probabilities_smooth\n",
    "\n",
    "for c in class_probabilities_smooth:\n",
    "    total_words = X_train[y_train.values == c].sum() + X_train.shape[1]\n",
    "    for w in range(X_train.shape[1]):\n",
    "        word_occurrences = X_train[y_train.values == c].toarray().sum(axis=0)[w] + 1\n",
    "    \n",
    "        class_probabilities_smooth[c][w] = (word_occurrences / total_words)\n",
    "    \n",
    "class_probabilities_smooth   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log((2 / (X_train[y_train.values==0].sum() + X_train.shape[1]))**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p_bad\n",
    "fake_array = zip(fake_review_transformed.data, fake_review_transformed.indices)\n",
    "for i, w in fake_array:\n",
    "    if i > 1:\n",
    "        p *= (class_probabilities_smooth[0][w]**i)\n",
    "        print(i)\n",
    "    else:\n",
    "        p *= class_probabilities_smooth[0][w]\n",
    "prob_bad = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p_good\n",
    "fake_array = zip(fake_review_transformed.data, fake_review_transformed.indices)\n",
    "for i, w in fake_array:\n",
    "    if i > 1:\n",
    "        p *= (class_probabilities_smooth[1][w]**i)\n",
    "        print(i)\n",
    "    else:\n",
    "        p *= class_probabilities_smooth[1][w]\n",
    "prob_good = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Review\n"
     ]
    }
   ],
   "source": [
    "if prob_good > prob_bad:\n",
    "    print(\"Good Review\")\n",
    "else:\n",
    "    print(\"Bad Review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/miniconda3/envs/unit2/lib/python3.7/site-packages/sklearn/naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(fake_review_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "For the Gaussian Naive Bayes, i'll be using the famed Iris dataset to provide continuous data for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (unit2)",
   "language": "python",
   "name": "unit2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
